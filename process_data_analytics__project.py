# -*- coding: utf-8 -*-
"""Process data Analytics _Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1flZIWOZKyvnyK0HxXuNZBHO5Sr1IG6wX

# **Exploratory Data Analysis (EDA)**
What EDA Is Doing


*   Understanding Data Structure: By previewing and summarizing, EDA helps you know what data you have and how it is organized.
*  Assessing Data Quality: Checking for missing values and data types ensures the dataset is clean and ready for analysis.
*   Describing Distributions: Summary statistics and histograms reveal how each variable behaves, whether there are outliers, and the general spread of the data.
*   Identifying Relationships: Scatter plots and correlation matrices help spot dependencies or associations between variables, which is crucial for feature selection and modeling.
*   Guiding Further Analysis: EDA provides insights that inform the next steps, such as which variables to focus on, potential data transformations, or hypotheses to test.

Prints the datasetâ€™s shape, column names, data types, count of missing values, and summary statistics for a quick overview.

*   Summary stats help you decide if the dataset needs preprocessing like scaling or outlier removal before feeding it into a model.
* **Summary stats help in deciding which features are useful for modeling**.- Features with very low variance may not add much value to prediction and might be dropped.
"""

# Step 1: Import packages
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display

# Step 2: Load the cleaned CSV
from google.colab import files
uploaded = files.upload()

# Read the uploaded file
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Step 3: Display info in styled tables
print(f"\n\033[1mDataset Shape:\033[0m {df.shape}")

# Column Names display
print(f"\n\033[1mColumn Names:\033[0m")
display(pd.DataFrame({'Column Names': df.columns}).style.set_properties(**{
    'background-color': '#E8F6F3'}).applymap(lambda val: 'color: black'))

# Data Types display
print(f"\n\033[1mData Types:\033[0m")
display(pd.DataFrame(df.dtypes, columns=['Data Type']).style.set_properties(**{
    'background-color': '#FEF9E7'}).applymap(lambda val: 'color: black'))

# Missing Values display
print(f"\n\033[1mMissing Values:\033[0m")
display(pd.DataFrame(df.isnull().sum(), columns=['Missing Count']).style.background_gradient(cmap='OrRd')
        .applymap(lambda val: 'color: black'))

# Summary Statistics display
print(f"\n\033[1mSummary Statistics:\033[0m")
display(df.describe().T.style.background_gradient(cmap='BuGn').format("{:.2f}")
        .set_caption("Summary Statistics").applymap(lambda val: 'color: black'))

"""ðŸ”´ Red for CV > 0.5 (high variability)

ðŸŸ  Orange for 0.3 < CV â‰¤ 0.5 (moderate)

ðŸŸ¢ Green for CV â‰¤ 0.3 (low variability)
"""

# Calculate Coefficient of Variation (CV) for each numeric column
cv_df = pd.DataFrame({
    "Mean": df.mean(numeric_only=True),
    "Std Dev": df.std(numeric_only=True),
})
cv_df["CV"] = cv_df["Std Dev"] / cv_df["Mean"]

# Sort by CV to see highest variation at top
cv_df = cv_df.sort_values(by="CV", ascending=False)

# Color-coding function
def highlight_cv(val):
    if val > 0.5:
        return 'background-color: #f28e8e; color: black'  # Red
    elif val > 0.3:
        return 'background-color: #f7d794; color: black'  # Orange
    else:
        return 'background-color: #c9f7c1; color: black'  # Green

# Display styled table
cv_df.style.format({
    "Mean": "{:.2f}",
    "Std Dev": "{:.2f}",
    "CV": "{:.2f}"
}).applymap(highlight_cv, subset=['CV'])

"""
1.   Visualizes missing data in the dataset using a heatmap. This helps you quickly spot columns/rows with missing values

2. A heatmap allows you to quickly identify which columns or rows have missing values


3.   If there are systematic patterns (e.g., missing data for certain categories or time periods), a heatmap can help you identify them quickly. This might influence how you handle missing data later
4.   By visualizing where the missing values are concentrated, you can decide whether you need to drop rows/columns, impute missing values, or apply other techniques for dealing with missing data."""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import KNNImputer
from IPython.display import display

# Set pandas to show all columns
pd.set_option('display.max_columns', None)

# Step 1: Load the dataset (replace with your file path or data loading method)
from google.colab import files
uploaded = files.upload()

# Read the uploaded file
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Step 2: Check for missing values in the dataset
print(f"\nMissing Values in the Dataset:\n")
print(df.isnull().sum())  # Number of missing values per column
print(f"\nPercentage of Missing Values:\n")
print(df.isnull().mean())  # Percentage of missing values per column

# Step 3: Visualize missing data (before handling)
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False, xticklabels=df.columns)
plt.title('Missing Values Heatmap Before Handling')
plt.show()

# Step 4: Handling Missing Values for Numeric Columns
numeric_columns = df.select_dtypes(include=['number']).columns

# Option 5: Impute missing values with the mean of the column (for numerical columns)
df_filled_mean = df.copy()
df_filled_mean[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Option 7: Impute missing values using KNN imputation for numerical columns
imputer = KNNImputer(n_neighbors=5)
df_filled_knn = df.copy()
df_filled_knn[numeric_columns] = pd.DataFrame(imputer.fit_transform(df[numeric_columns]), columns=numeric_columns)

# Step 5: Display the dataset after handling missing values (before heatmap)
print(f"\nData After Imputation (Mean Imputation for Numerical Columns):\n")
display(df_filled_mean.head())  # Show the first few rows after imputation

# Visualize missing data after handling
plt.figure(figsize=(12, 6))
sns.heatmap(df_filled_mean.isnull(), cbar=False, cmap='viridis', yticklabels=False, xticklabels=df_filled_mean.columns)
plt.title('Missing Values Heatmap After Handling')
plt.show()

"""**1**. A correlation heatmap is a graphical representation of the correlation matrix, showing how different variables in a dataset are related to one another.
2. **Identifying Relationships Between Variables:**
Correlation measures the statistical relationship between two variables. It ranges from -1 to 1:


*   1 indicates a perfect positive correlation.
*   -1 indicates a perfect negative correlation.
*  0 indicates no correlation.
3. Feature selection: If two variables are highly correlated, one might be redundant, and removing one could simplify the model without losing much information.
4. Multicollinearity detection: High correlation between features can lead to multicollinearity, which can affect the performance of certain machine learning algorithms, like linear regression.






"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import KNNImputer
from IPython.display import display

# Set pandas to show all columns
pd.set_option('display.max_columns', None)

# Step 1: Load the dataset (replace with your file path or data loading method)
from google.colab import files
uploaded = files.upload()

# Read the uploaded file
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Step 2: Check for missing values in the dataset
print(f"\nMissing Values in the Dataset:\n")
print(df.isnull().sum())  # Number of missing values per column
print(f"\nPercentage of Missing Values:\n")
print(df.isnull().mean())  # Percentage of missing values per column

# Step 3: Handle missing values for numeric columns
numeric_columns = df.select_dtypes(include=['number']).columns

# Option 5: Impute missing values with the mean of the column (for numerical columns)
df_filled_mean = df.copy()
df_filled_mean[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Option 7: Impute missing values using KNN imputation for numerical columns
imputer = KNNImputer(n_neighbors=5)
df_filled_knn = df.copy()
df_filled_knn[numeric_columns] = pd.DataFrame(imputer.fit_transform(df[numeric_columns]), columns=numeric_columns)

# Step 4: Display the dataset after handling missing values (before correlation heatmap)
print(f"\nData After Imputation (Mean Imputation for Numerical Columns):\n")
display(df_filled_mean.head())  # Show the first few rows after imputation

# Step 5: Calculate the Correlation Matrix (only numeric columns)
corr_matrix = df_filled_mean[numeric_columns].corr()  # Correlation matrix for numeric columns

# Step 6: Plot the Correlation Heatmap
plt.figure(figsize=(12, 8))  # Set figure size
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar_kws={'shrink': 0.75})
plt.title("Correlation Heatmap of the Dataset (After Imputation)", fontsize=16)
plt.show()

import pandas as pd
import numpy as np

# Assuming you already have the dataframe loaded as 'data'
# If not, load it first:
# data = pd.read_csv('kamyr-digester.csv')

# Get numeric columns (excluding datetime-like 'Observation')
numeric_cols = data.select_dtypes(include=np.number).columns.tolist()

# Calculate Coefficient of Variation (Cv)
cv_df = pd.DataFrame({
    'Mean': data[numeric_cols].mean(),
    'Std Dev': data[numeric_cols].std(),
})
cv_df['Cv'] = (cv_df['Std Dev'] / cv_df['Mean']).abs()

# Display sorted results with styling
cv_df_sorted = cv_df.sort_values(by='Cv', ascending=False)
cv_df_sorted.style.background_gradient(cmap='YlOrRd').format("{:.2f}").set_caption("Coefficient of Variation (Cv)")

"""Plots the distribution (histogram + KDE) for the first six columns (you can adjust which columns to use). This shows the spread and skewness of each variable.

plotting the frequency of each variable is a fundamental step in exploratory data analysis because it provides a clear, visual overview of your dataâ€™s structure, highlights key patterns, and guides subsequent analysis and decision-making
"""

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Upload the file manually from your local system
uploaded = files.upload()


# List of columns to exclude from input variables
exclude_cols = ['timestamp', 'hour', 'y_kappa']  # Add any other output/non-input columns if necessary

# Select input variables: all numeric columns except those in exclude_cols
input_vars = [col for col in df.select_dtypes(include='number').columns if col not in exclude_cols]

print("Input variables being plotted:", input_vars)

# Plot distribution for each input variable
for col in input_vars:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

"""Plots boxplots for the same key variables, making it easy to spot outliers."""

import matplotlib.pyplot as plt
import seaborn as sns

# List of columns to exclude from input variables
exclude_cols = ['timestamp', 'hour', 'y_kappa']  # Adjust based on your dataset

# Select only numeric input variables
input_vars = [col for col in df.select_dtypes(include='number').columns if col not in exclude_cols]

print("Input variables being plotted:", input_vars)

# Create box plots for each input variable
for col in input_vars:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Box Plot of {col}')
    plt.xlabel(col)
    plt.tight_layout()
    plt.show()

"""plot will show the actual time series for y_kappa from the data.

#  **2. Feature Engineering**
Create new features to improve model performance or provide more insights.

Lag features (for time series patterns)

Rolling means/medians

Temperature differentials, e.g., delta_temp = upper_ext - lower_ext

Rate of change (Î” mass, Î” temp)

Hour of day or shift classification

Feature engineering is the process of transforming raw data into features that can be effectively used by machine learning models. The main uses and benefits of feature engineering include:

Improving Model Accuracy: By creating new features or transforming existing ones, feature engineering helps models better capture important patterns and relationships in the data, leading to more accurate predictions.

Making Data Machine-Readable: Many raw datasets are not in a format suitable for machine learning algorithms. Feature engineering preprocesses data into a machine-readable format, optimizing it for the specific requirements of different algorithms.

Uncovering Hidden Patterns: Creating new variables (features) can reveal hidden relationships or trends in the data that were not obvious in the raw form, enhancing the modelâ€™s ability to learn from the data.

Handling Data Challenges: Feature engineering addresses issues such as missing data, high dimensionality, and categorical variables, making the dataset more robust and easier to analyze.

Reducing Model Complexity and Overfitting: By selecting or creating the most relevant features, feature engineering can reduce the dimensionality of data, which simplifies models and helps prevent overfitting.

Incorporating Domain Knowledge: It allows you to integrate expert or domain-specific knowledge into the dataset, making features more meaningful and interpretable for the problem at hand.

Speeding Up Modeling Process: Well-engineered features can make the training process faster and more efficient, as the model can focus on the most relevant information.

Enhancing Model Interpretability: Creating features that are meaningful in the context of the business or domain makes it easier to understand and explain model predictions
"""



import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from IPython.display import display

# Load the data
df = pd.read_csv('kamyr-digester.csv')
df.columns = df.columns.str.strip()  # Clean column names

# Identify numeric columns for imputation and scaling (excluding Observation/timestamp)
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Impute missing values in numeric columns with the mean
imputer = SimpleImputer(strategy='mean')
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

# Feature Engineering: Create interaction feature if both columns exist
if 'ChipRate' in df.columns and 'BlowFlow' in df.columns:
    df['ChipRate_BlowFlow_Interaction'] = df['ChipRate'] * df['BlowFlow']
    numeric_cols.append('ChipRate_BlowFlow_Interaction')

# Standardize numeric features (including the new interaction feature)
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Display the first 300 rows as a styled DataFrame in Jupyter
styled = df.head(5).style.background_gradient(cmap='viridis')
display(styled)

# If running as a script, you can also save or print:
# df.to_csv('kamyr_digester_engineered.csv', index=False)
# print(df.head())

"""**Identifies critical process parameters affecting pulp quality (Y-Kappa)**

1. Plotting "Y-Kappa" against various process inputs can provide insights into the relationship between the key performance metric (Y-Kappa) and the different variables involved in the process.
2. The scatter plots help visualize how Y-Kappa (likely a key output or quality indicator in your process) behaves relative to different process parameters (inputs). If there's a noticeable trend (e.g., linear, quadratic), it suggests that changes in the input could affect the Y-Kappa outcome.

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("darkgrid")
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE


# Load and clean data
data = pd.read_csv('kamyr-digester.csv')
data.columns = data.columns.str.strip()

# Define process inputs and key metrics
process_inputs = [
    'ChipRate', 'BF-CMratio', 'BlowFlow', 'ChipLevel4', 'T-upperExt-2', 'T-lowerExt-2', 'UCZAA', 'WhiteFlow-4',
    'AAWhiteSt-4', 'AA-Wood-4', 'ChipMoisture-4', 'SteamFlow-4', 'Lower-HeatT-3', 'Upper-HeatT-3',
    'ChipMass-4', 'WeakLiquorF', 'BlackFlow-2', 'WeakWashF', 'SteamHeatF-3', 'T-Top-Chips-4', 'SulphidityL-4'
]
key_metrics = ['Y-Kappa', 'BlowFlow', 'SteamFlow-4', 'ChipMoisture-4', 'ChipRate']

# 1. Y-Kappa vs various process inputs (scatter plots)
plt.figure(figsize=(18, 24))
for i, col in enumerate(process_inputs):
    plt.subplot(7, 3, i+1)
    sns.scatterplot(x=data[col], y=data['Y-Kappa'], color='royalblue', edgecolor='k', alpha=0.7)
    plt.title(f'Y-Kappa vs {col}', fontsize=11)
    plt.xlabel(col, fontsize=9)
    plt.ylabel('Y-Kappa', fontsize=9)
    plt.grid(True, linestyle='--', linewidth=0.5)
    # Optionally add a regression line
    if data[col].notnull().sum() > 10:
        sns.regplot(x=data[col], y=data['Y-Kappa'], scatter=False, color='crimson', line_kws={'linewidth':1})
    plt.tight_layout()
plt.suptitle('Y-Kappa vs Process Inputs', fontsize=16, y=1.02)
plt.show()